# [Chaper 1] 대규모 웹 서비스 개발 오리엔테이션 - 전체 그림 파악하기

## 이 책을 들어가기 전…

### 저자가 전달하고 자 하는것

- 대규모 웹 서비스 ? 개발 ?
- 대규모 데이터를 다룰 때의 과제, 기본적인 사고방식과 요령
    - OS 캐시 / DB 운용방법 등
- 알고리즘과 데이터 구조 선택의 중요성
- RDBS로 모두 다룰 수 없는 규모의 데이터 처리방법
- 대규모 서비스가 될 것을 전제로 한 서버/인프라의 예와 개념

### 책의 구성

| 챕터  | 내용 |
| --- | --- |
| 제 1장 | 하테나로 알아보는 대규모 서비스 개요, 규모, 어려운점, 개발 모습 등 |
| 제2장 - 제 5장 | 대규모 데이터를 다루는 애플리케이션 개발에 필요한 지식  |
| 제6장 - 제 10장 | 지식을 바탕으로 구현해보는 단계 (과제) |
| 제 11장 - 제 15장 | 오픈 소스 중심의 대규모 환경으로 확장성을 갖게 하기 위한 인프라 구성 |

⇒  **대규모 서버스, 대규모 데이터를 다룰 경우를 대비한 기본적인 사고방식과 개념, 개요에 한해서 전달** 

⇒  대규모 데이터 처리뿐만 아니라 기술습득에서도 중요한 것은 How-To 습득이 아니라 **밑바탕이 되는 전체 그림을 파악**

### 🗣️웹 서비스가 미래에 성장해 있다면 어떻게 해야 좋을까?!

## 대규모 서비스와 소규모 서비스

### 대규모  웹 서비스

```java
- 거대한 데이터를 처리해야만 하는 웹 서비스 
- 매우 많은 수의 사용자와 데이터를 처리하는 큰 규모의 웹 기반 응용 프로그램
```

---

### **하테나의 서비스 규모 (2009년 기준)**

- 등록 사용자는 100만 명 이상, 1,500만 Unique User, 고유 사용자)/월
- 수십 억 액세스/월 (이미지 등으로의 액세스는 제외)
- 피크(peek)시 회선 트래픽 양은 430Mbps
    - 특정 시간 동안 전송되는 데이터의 양이 가장 높은 지점
    - 회선을 통해 전송되는 데이터의 양이 최고로 높았고, 그 값이 430Mbps라는 것 ⇒ 최대 처리량을 경험
    - 회선 트래픽은 네트워크에서 전송되는 데이터 양을 측정하는데 사용되는 지표
    - 네트워크 장비(라우터, 스위치 등)를 통해 흐르는 데이터의 양을 측정
    
    ---
    
    - Mbps(Mega bi per second)는 초당 백만 비트(bit)를 전송하는 비트 전송 속도를 나타내는 단위
        - ex) 예를 들어, 430Mbps의 트래픽이 1분 동안 지속된다면, 전송된 데이터 양은 다음과 같이 계산될 수 있다.
        - (430 Mbps) × (60초) = 25,800 메가비트 (약 25.8 기가비트)
        - 430Mbps의 피크 트래픽은 **약 25.8 기가비트**의 데이터를 전송한 것으로 해석
- 하드웨어(서버)는 500대 이상

⇒  따라서 이 정도 규모의 액세스가 되면 일일 액세스 로그는 기본적으로 기가바이트 크기. 데이터를 처리하는 서버는 당연히 1대로는 불가능 , 약 500대 이상. 이는 20랙 이상으로 서버를 관리.

⇒ 1대의 서버에는 복수의 호스트가 가동. 결과적으로 호스트 수는 1,000대를 넘는다. (책의 후반에서 자세히 설명)

---

### 하테나는 대규모, 구글 및 페이스 북은 초대규모

- 서비스의 규모는 서버대수 등으로 개략적으로 파악되는 경우가 많다. 이런 관점에서 바라볼 때 **100대에서 수천 대 정도가 대규모 서비스라고 할 수 있다고 한다.**
- 구글 및 해외에서 인기 있는 SNS, 페이스북과 같은 세계 Top 클래스 사이트는 서버 대수가 **수백만 대 규모**이고, **처리하는 데이터는 테라바이트 ~ 페타바이트(petabyte)급의 초대규모 서비스다.**

---

### 소규모 서비스와 대규모 서비스의 차이

<aside>
❓ 서버 몇 대 정도의 소규모 서비스에는 없는**, 대규모 서비스에만 있는 문제나 어려움**에는 어떤 점들이 있을까?

</aside>

> **확장성 확보, 부하분산 필요**
> 

⚠️ 대량의 액세스가 있는 서비스에서 서버 1대로 처리할 수 없는 부하를 어떻게 처리할 것인지가 가장 큰 문제

**스케일 아웃 (Scale-Out)**

- 서버를 **횡**으로 전개, 즉 서버의 역할을 분담하거나 대수를 늘림으로써 시스템의 전체적인 처리능력을 높여 부하를 분산하는 방법
- 저가의 하드웨어를 횡으로 나열해서 **확장성을 확보**하는것이 전략
- 서버가 여러 대가 되기 때문에 각 서버에 걸리는 부하를 균등하게 해주는 ‘로드벨런싱’이 동반되어야한다.
- 서버가 한 대가  장애로 다운되더라도 다른 서버로 서비스 제공이 가능다는 장점을 가지고 있음
- 비용이 절감되지만 다양한 문제들이 발생한다. (서버가 1대였다면 고려할 필요가 없겠지)
    - 사용자로부터의 요청을 어떻게 분배할 것인가?
    - 데이터 동기화는 어떻게 할 것인가? ex)  DB 동기화 문제
    - 네트워크 통신의 지연시간을 어떻게 생각해볼 수 있을까? 등

**스케일 업 (Scale-Up)**

- 하드웨어의 성능을 높여 처리능력을 끌어올리는 방법
- 하드웨어의 성능과 가격은 비례하지 않는다.

![스크린샷 2023-07-11 오후 5.34.21.png](%5BChaper%201%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%8B%E1%85%B0%E1%86%B8%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%B5%E1%84%89%E1%85%B3%20%E1%84%80%E1%85%A2%E1%84%87%E1%85%A1%E1%86%AF%20%E1%84%8B%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8B%E1%85%A6%E1%86%AB%E1%84%90%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%20%200612ced05b25490b9a1ae9c5bf226e4f/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-07-11_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.34.21.png)

> **다중성 확보**
> 

⚠️ 특정 서버가 고장 나거나 성능이 저하되더라도 서비스를 계속할 수 있는 구성으로 할 필요가 있다.

- 스케일아웃 → 서버 多 , 고장률 **↑ :** 24시간 , 365일 돌아가야하는 웹 서비스에게는 치명적인 문제
- 서버가 대규모화 되면 될수록 시스템 정지의 사회적 충격도 늘어나므로 더욱 더 다중성 확보가 중요
- ex ) 2001년 9월 테러 당시 , 야후에 액세스에서 페이지가 다운되는 일이 있었음 → CDN 서비스인 Akamai에 컨텐츠를 캐싱해서 트래픽을 우회시켜 장애를 복구

> **효율적 운용 필요**
> 
- 서버가 1대라면 때때로 상태를 확인할 수 있지만 , 100대를 넘어서면 서버가 무슨 역할을 하고 있는지, 각 서버가 어떤 상황에 있는지 파악하기 어려움 .
    - ex) 부하는 괜찮은지, 고장 난 부분 없는지, 디스크 용량은 충분한지, 보안설정 미비 등
- 감시용 소프트웨어를 사용하고 정보관리를 위한 툴을 사용하는 등 자동화하지만, 이 감시 소프트웨어를 설치하거나 정보를 보는건 결국 인간이 된다. 일손을 거치지 않고 대규모 시스템을 건강한 상태로 얼마나 계속 유지할 수 있을까?

⇒ **따라서 이를 위한 효율적 운용을 수행해야한다.**

> **개발자 수, 개발방법의 변화**
> 

⚠️혼자는 개발이나 운용이 어려우니 여러 기술자가 역할을 분담 , 하지만 제멋대로 개발하거나 표준화가 없다면?  이 또한 규모가 커질 수록 문제가 발생

- 프로그래밍 언어를, 라이브러리 프레임워크를 통일
- 코딩 규약을 정해서 표준화하고, 코딩 규약을 정해 표준화 , 소스코드 관리를 버전 관리 시스템으로 이동
- 팀 매니지먼트가 필요해지는 것

---

### 대규모 데이터 처리량에 대한 대처

- 디스크 → 메모리 → 캐시 메모리 → CPU
- 각 단계 간에는 속도차가 매우 크게 나타남
- 하드디스크에서 데이터를 읽어오는 것과 , 전기적으로 읽어들이기만 하면 되는 메모리나 캐시 메모리와 비교하면 10^6 ~ 10^9 배나 되는 속도차가 나게된다.
    - 이러한 속도차를 흡수하기 위해  OS는  디스크로 부터 읽어들인 데이터를 메모리에 캐싱해두기도 함 (DB를 비롯한 미들웨어에서도 채용)

![스크린샷 2023-07-11 오후 6.06.01.png](%5BChaper%201%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%8B%E1%85%B0%E1%86%B8%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%B5%E1%84%89%E1%85%B3%20%E1%84%80%E1%85%A2%E1%84%87%E1%85%A1%E1%86%AF%20%E1%84%8B%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8B%E1%85%A6%E1%86%AB%E1%84%90%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%20%200612ced05b25490b9a1ae9c5bf226e4f/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-07-11_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.06.01.png)

⚠️ 하지만, 이는 당연히 한계가 있다. 데이터량이 많아지면 처음부터 **캐시 미스(cache miss)가 많이 발생**하게됨 → **DISK I/O** **↑**  →  **시스템 전체의 속도 저하.**

### 따라서

⇒ 어떻게 하면 데이터를 적게 가져갈 수 있을까 ?

⇒ 여러 서버로 분산시킬 수 있을까 ?

⇒ 필요한 데이터를 최소한의 횟수로 읽어들일 수 있을까 ? 등 이 본질적인 과제이다.

## 계속 성장하는 서비스와 대규모화의 벽

### 웹 서비스의 어려움

<aside>
💡 웹 서비스가 어려운 이유 중 또 하나의 원인은 **서비스가 계속 성장한다는 점이다.**

</aside>

- 소규모였던 서비스가 성장함에 따라 시스템 구성을 변화시켜야한다.
- 데이터량의 증가, 트래픽 증가, 액세스 횟수 증가

---

### 하테나가 성장하기까지

> **초기**
> 
- 2001sus 당시 , 초기 시스템 펜티엄 3 PC 1대
- 회선 : ADSL

> **시행착오를 거듭한 시스템 규모 확장**
> 
- 라우터는 Linux 저가에 구축
- HTTP 요청 분산은 아파치의 mod_rewrite로 대용
- DB분산은 불안정했던 MySQL의 레플리케이션

**하지만,,,** 

- 시스템이 붐이 일어나자 사이트가 무거워지고 엑세스 불가능 → 안정성 , 다중화 , 효율적 운영에서 낙제점
- 서버 증설에 따라 전력 부족

> **데이터 센터로의 이전, 시스템 쇄신**
> 
- 하테나는 조직상으로 아직 미숙하여 계속 증가하는 트래픽이나 부하에 계획적으로 대응할 수 없었음 → 조직체제 재점검 , 시스템 운용 전담팀 구축 후 대응 시작
- 작은 서버 룸 → 인터넷 데이터 센터로 이전
- 기존 시스템의 부하상황 정리 : 병목지점 측정 및 판정 ,  I/O 부하가 높은 서버는 메모리 중시,CPU 부하가 높은 서버는 CPU를 중요시하는 형태로 서버 용도에 맞게 최적의 구성을 갖는 하드웨어 준비
- 다중화 :  로드밸런서 + 가동감시 기능 오픈소스인 LVS + keepalived 도입 + Linux 박스로 구축한 로드밸런서 도입
- 서버 교체 : OS 가상화 진행해서 서버 가동률 높임
- 서버의 정보관리 : 웹 기반 서버 정보관리 시스템 개발 → 각종 서버의 용도나 부하상태와 같은 정보에 액세스
- 서버/인프라 측면의 시스템 구성 및 애플리케이션의 각종 로직이나 DB 스키마 재검토하여 비효율적인 부문을 배제

### 시스템 성장 전략 - 미니멈 스타트 , 변화를 내다본 관리와설계

- 소규모 단계에서 너무 이른 최적화는 좋은 방침이라 할 수없다.
- 처음부터 완벽한 부하분산 시스템을 구축은 많은 비용을 들게 한다.
- 이러한 사태가 발생하지 않게 하기 위해 어느정도의 수용능력 관리나 서비스 설계시에 필요 이상으로 데이터를 증가시키지 않도록 설계하는 **미니멈 스타트**가 좋다.

## 서비스 개발의 현장

### 하테나의 기술팀 채제

- `서비스 개발부` : 하테나의 각종 서비스를 구현을 담당. 애플리케이션 측면의 개선
- `인프라` : 서버/인프라 시스템의 운용을 담당. 서버 준비, 데이터 센터 운용 , 부하분산 담당 등

### 하테나에서의 커뮤니케이션 방법

- `하테나 그룹`:  블로그와 위키를 조합한 그룹웨어 : 업무 리포트, 블로그 등으로 이용
- `IRC` : 채팅 커뮤니케이션 → 유지보수 진행상황이나 긴급 문제 전달. 장애 통지 실시간 정보 공유
- `서버 관리 툴`  : 현재 서버 상황을 한눈에 알 수 있는 툴 → 유지보수 예정 유무 확인 및 시스템 갱신 가능 판단

### 실제 서비스 개발

1. 매일 아침 팀 별로 10분 미팅 : 진척상황, 오늘 할일 공유 등
2. 테스트 담당자 정하고 , 태스크 구현
3. 구현에 있어서 가능한 한 테스트 프로그램을 작성
4. 테스트 프로그램을 작성한 후 구현. 구현이 완료되변 버전관리 시스템(git)에 커밋
5. 구현 후 , 코드리뷰 → 사내 코딩규약에 따르지 않는 코드나 과부하 유발될 만한 것들을 미리 방지
6. 리뷰가 지나면 실제로 동작하고 있는 시스템 환경(프로덕션 환경)의 코드에 머지를하고 동작확인용 환경에서 동작을 확인한 후 프로덕현 시스템에 반영
7. 기타로 페어프로그래밍 , 애자일 개발 스타일 등

### 개발에 사용되는 툴

- `프로그래밍 언어` : 동일 레이어인 언어는 하나로 선정. 같은 언어를 사용하면 자사 내에서 노하우가 널리 통용되고 팀 간 이동 원활. 유지보수에도 용이
- `주요 미들웨어` : 표준화 관점에서 이용할 미들웨어와 프레임워크도 통일
- `웹 애플리케이션 프레임워크` : 애플리케이션 개발의 효율을 높이기 위해, 또한 표준화를 진행하려는 의미로 프레임워크를 이용
- `주요 머신의 OS 및 에디터` : PC나 OS는 자유. 대신 vmware나 coLinux와 같은 가상 os를 각 개발자가 도입해서 그 위에서 사용 , 에디터는 자유지만 코딩 규약으로서 Emacs나 Vim 을 사용하여 하테나 규약대로 정형화해주는 설정 도입
- `버전 관리` : git, BTS는 아시카
- `개발 툴` : 대규모 개발에 있어서는 고기능인 툴을 선택하기보다도 얼마나 효율성을 희생하지 않고 표준화하고 , 자바의 워크플로우에 맞게 사용할 수 있는다를 더 중요하게 생각

## 정리

- 1장에서는 대규모 웹 서비스의 개발 모습으로 여기서 발생하는 문제를 살펴보았고 , “대규모”라는 이미지화 하기 위해 하테나의 실제모습을 소개
- 이후 강의에서 구체적으로 **대규모 데이터 공략방법과 대규모 시스템의 구성방법**을 설명

## 참고 블로그 및 자료

[사용자매뉴얼 : 웹 로그분석 서비스 로거](https://logger.co.kr/manual_FR/help.tsp?node=643)

[[DB] DB확장을 하는  두가지 방법- 스케일 아웃(scale out)과 스케일 업 (scale up)](https://devuna.tistory.com/73)

## 1장 용어 정리

`Unique User`

- 웹사이트에 접속한 - Unique user - 유일한 사용자의 뜻을 가지고 있으나, 사용자를 개별 인증을 하지 않는 한 정확한 사용자의 수를 측정하기는 불가능 하며, 일반적으로 [Unique session](https://logger.co.kr/manual_FR/help.tsp?node=644&upperNode=607)과 같은 의미로 자주 사용됩니다.
- [Unique session](https://logger.co.kr/manual_FR/help.tsp?node=644&upperNode=607)은 웹로그 분석에 있어서는 웹사이트에 접속한 사용자 중, 접속하여 웹서버와 데이터를 주고 받는 행동이 끊기지 않은 채로(30분간 어떠한 데이터 전송이 없으면 끊긴것으로 간주), 웹브라우징을 지속한 사용자의 수를 나타냅니다.
- 보통 쿠키([Cookie](https://logger.co.kr/manual_FR/help.tsp?node=625&upperNode=601))를 통하여 사용자의 세션([Session](https://logger.co.kr/manual_FR/help.tsp?node=645&upperNode=607))을 추적하며, 이를 통해 [Page View](https://logger.co.kr/manual_FR/help.tsp?node=638&upperNode=606) - 페이지뷰가 아닌 접속자의 수(사용자의 수)를 가늠하게 됩니다.
- 이러한 Unique user([Unique session](https://logger.co.kr/manual_FR/help.tsp?node=644&upperNode=607))이 아닌 [IP Address](https://logger.co.kr/manual_FR/help.tsp?node=633&upperNode=603)를 이용하여서도 사용자수를 가늠하기도 하며, 이 경우의 차이는 IP는 고정적으로써, 만약 공공의 컴퓨터를 오전에 1회 사용, 오후에 1회 사용한다면, IP수는 1개로 측정되며 Unique User수는 2개로 측정됩니다.

`랙`

- 랙(Rack)은 PC나 서버, 통신장비, 각종 계측기 등 일정 시스템을 구성하는 장비들을 보관하고 시스템 구성에 필요한 환경을 만들어주는 제품

![스크린샷 2023-07-11 오후 5.16.32.png](%5BChaper%201%5D%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%8B%E1%85%B0%E1%86%B8%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%B5%E1%84%89%E1%85%B3%20%E1%84%80%E1%85%A2%E1%84%87%E1%85%A1%E1%86%AF%20%E1%84%8B%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8B%E1%85%A6%E1%86%AB%E1%84%90%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%20%200612ced05b25490b9a1ae9c5bf226e4f/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-07-11_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.16.32.png)

`캐시 미스(cache-miss)`

- CPU가 데이터를 요청하여 캐시 메모리에 접근했을 때 캐시 메모리가 해당 데이터를 가지고 있다면 이를 '캐시 적중(cache hit)'이라고 부르고, **해당 데이터가 없어서 DRAM에서 가져와야 한다면** '캐시 부적중(cache miss)'라 부른다.

`MySQL Replication`

- 리플리케이션(Replication)은 복제를 뜻하며 2대 이상의 DBMS를 나눠서 데이터를 저장하는 방식이며, 사용하기 위한 최소 구성은 **Master / Slave 구성**을 하여여 한다.
- 자세히
    
    [MySQL Replication(복제) - 단방향 이중화](https://server-talk.tistory.com/240)
