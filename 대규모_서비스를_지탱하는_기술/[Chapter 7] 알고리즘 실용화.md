# [Chapter 7] 알고리즘 실용화

## Chapter 7의 목표

<aside>
📖 알고리즘과 그 응용에 대한 개론

</aside>

- 대규모 데이터 처리 시간을 **문제해결에 적합한 알고리즘과 데이터 구조를 사용**함으로써 줄일 수 있다.
- 대규모 데이터를 처리할 때의 알고리즘 선택의 중요성과 알고리즘을 이용하여 제품으로 만들어내기까지 어떤 과정을 거치는지 이해해보자
- 헤테나 북마크 /  하테나 카테고리

## 알고리즘과 평가

### 데이터 규모와 계산량의 차이

---

- 대상이 되는 데이터가 크면 클수록 **알고리즘이나 데이터 구조 선택이 속도에 영향을 미친다.**
- 예) 1000건 : 선형탐색O(n)은 최대 1,000번 탐색 / 이분탐색 O(log n)은 **최대 10건**
- 예) 1000만건 : 선형탐색O(n)은 최대 1,000만번 탐색 / 이분탐색 O(log n)은 **최대 24건**
- 이 최대 탐색횟수는 계산횟수의 기준이 되는 수로 `계산량` 이라고 한다. 계산량이 적을 수록 속도가 빠르다.

### 알고리즘이란?

---

<aside>
📖 알고리즘은 어떤 값 또는 값의 집합을 입력(input)으로 하고 어떤 값 또는 값의 집합을 출력(output)으로 하는, 명확하게 정의된(well-defined) 계산절차다.

</aside>

- 적당한 값을 입력하면 명확하게 정의된 계산 절차에 따라 값이 출력으로 반환되는 것
- `넓은` 의미의 알고리즘: 처리(도메인 로직)의 흐름
- `좁은` 의미의 알고리즘: 명확하게 정의된 계산 문제에 대해 정의된 계산절차를 수행하는 것

### 알고리즘을 배우는 의의

---

- CPU나 메모리 등 컴퓨터의 **유한한 자원으로 어떻게 해야되는가**에 익혀둘 필요가 있다.
- 알고리즘 또한 디자인패턴과 마찬가지로 엔지니어에게 공통언어이다. 즉 **커뮤니케이션**을 위해서도 올바르게 이해해둘 필요가 있다.
- 새로운 문제에도 대처할 수 있다.
    - ex) 베이지안 필터를 실현하는 알고리즘을 알면, 자동분류하는 프로그램을 작성할 수있다. 또 메일 스팸필터를 만들 수 있음
    - 수억 건의 레코드를 수 MB로 저장할 수 있는 데이터 구조가 있다면 배포하기 너무 컸던 프로그램을 부담없이 배포할 수 있게된다.
- **대규모 데이터를 앞둔 경우, 알고리즘 측면에서의 특성이 애플리케이션의 성능에 큰 영향을 주기도 한다.**

### 알고리즘의 평가

---

- Order 표기 : 대상이 되는 알고리즘이 입력 크기가 n일때 대략적인 계산량이 쇼요된다는 것을 표기하는 기법
- 알고리즘의 평가에는 Order 표기를 사용하는 것이 일반적
- Order표기는 특정 상황을 다루는 것이 아니라 평균 또는 최대를 평가한다.
- 알고리즘의 계산량은 대개의 경우 정량적으로 평가할 수 있다.

```java
O(1) < O(log n) < O(n) < O(n logn) < O(n^2) < O(n^3) < .... O(n^k) < O(2^n)
```

- 대규모 데이터를 대상으로 한 경우, 즉 n이 클경우 **실질적으로 실용성을 띄는 것은 O(n logn)부근**까지 이다.
- 계산량 개념은 계산시간뿐만 아니라 **공간적인 양에서도 사용된다**. 메모리 사용량을 논할 경우에도 해당한다.

### 알고리즘에 있어서 지수적, 대수적 감각

---

- 계산량이 **지수적**으로 증가하는 알고리즘은 데이터량이 적어도 **계산량이 매우 커진다.**
- 반대로 지**수의 역인 대수적으로만 증가하는 O(log n)인 알고리즘**은 데이터량이 꽤 커져도 **적은 계산량**으로 문제를 해결할 수 있다.
- **이런 감각을 아는 것이 중요하다**. O(n^2)나  O(2^n)를 잘못선택한다면 수백건 정도의 데이터를 대상으로 하더라도 상당히 불필요한 리소스를 사용하게되버린다.

### 알고리즘과 데이터 구조

---

- 데이터 구조 : 배열, 트리구조와 같이 대상이 되는 데이터를 저장 또는 표현하기 위한 구조를 말한다.
- 데이터 구조와 알고리즘이 세트로 논의되는 것은 **알고리즘에서 자주 사용하는 조작에 맞춰 데이터 구조를 선택할 필요가 있기 때문**이다.
    - 트리구조를 통해 탐색처리를 단순화
    - RDBMS에서의 인덱스를 구현할때 B+트리라는 트리구조가 사용

### 계산량과 상수항

---

- 계산량의 Order 표기에서는 이른바 `상수항`을 무시한다.
- `상수항` : 해당 알고리즘을 구현하는 중에 입력 크기에는 의존하지 않지만 실행하지 않으면 안되는 처리의 일종
    - 함수호출이나 함수로부터 값을 반환하기 위한 처리
    - 일차변수를 확보
    - if문으로 분기시키는 등의 처리
- 간단한 구현에서는 상수항이 계산량에 영향을 미치지 않지만, **복잡한 구현이 되면 상수항을 무시할 수 없다.**
- 구현이 복잡하지 않더라도 CPU캐시에 올리기 쉬운지, 분기예측이 발생하지 않는지 등 계산량의 구조적인 특성에 의존하는 형태로 상수항에서 차이가 날 수 있다.
    - 정렬 알고리즘 O(n logn)이 하한. 평균 계산량이 O(n logn)인 알고리즘은 여러개지만 그중 퀵 정렬이 가장 빠르다고한다.
    - 퀵 정렬은 그 특성상 CPU 캐시를 사용하기 쉽다는 장점이 있어 이 점이 비교할 때 유리하게 작용한다. 이는 상수항이 빠르다는 말
- 즉 **Order 표기는 알고리즘을 비교할 때 편리하지만 구현을 포함해서 생각할때 그게 전부는 아니다.**
- **상수항은 어떻게 구현하느냐에 의존하는 경우가 많으므로 이를 줄이기 위해서 구현하는데 노력이 필요하다.**

### 구현 시 유의하고픈 최적화 이야기

---

- **알고리즘뿐만 아니라 뭔가를 구현할 때 상수항을 줄이기 위해 처음부터 최적화를 수행하는 것은 옳지 않다.**
    - o(n^2) 로 상수항 줄이는걸 했는데 나중에 O(n logn)이 있을 수 있음 당연히 후자가 더 좋을듯)
- `측정`이 중요하다.
    - 벤치마크를 하거나 프로파일링을 해서 지금 대상으로 하고 있는 프로그램에 무엇이 문제인지를 파악하고 , 알고리즘을 개선해야되는지 상수항을 줄여야하는지 판단해야한다.
- 규명한 후에 개선하려는 자세를 잊지말자

### 알고리즘의 실제 활용

---

- **고도의 알고리즘이 반드시 최고의 해법은 아니며, 고전적인 알고리즘(or 잘 알려진 알고리즘)이 좋은 경우도 있다.**
- 예 ) 하테나 북마크 Firefox 확장 기능인 검색기능에서 시행착오
    - `기능` : 과거에 사용자 본인이 북마크한 데이터를 증분검색(Incremental Search)하는 기능
    - `기존` : 팀 내에서 의논하여 ‘증분검색이면 검색도 상당한 빈도로 발생하고 클라이언트에서 계산되기 때문에 계산량을 적게 가져가기 위해 데이터량을 사람에 따라 1만 건 이상 가정하고 Suffix Array를 사용하자’ 로 생각함
    - 따라서 IS기법으로 구현, 하지만 속도는 나더라도 전처리에는 얼마간의 시간이 걸리고 사용자가 북마크 할 때마다 전처리 수행하도록 하니 머신에 부하가 발생
    - `결론` : Suffix Array 채택을 취소 → Firefox 확장기능이 내부적으로 갖고 있는 SQLite에 SQL 로 Like에 의한 부분일치(선형탐색)을 수행하도록 함
    - 데이터량이 많은 사람에게는 속도저하가 발생할거라고 생각했지만 완성해보니 전혀 문제가 없었음
    - 걱정했떤 데이터량 부분도 수만건 정도면 요즘 컴퓨터의 현저한 성능향상 덕분에 아무 문제 없이 탐색가능했다.
- 따라서 중요한 점은 **예측이나 측정이 중요**하다는 점 , 따라서는 명쾌하게 단순한 구현을 시도해보는것도 좋다.
- 대규모 데이터를 상정한 최적화라는 것도 중요하지만 , 여기서 살펴보았듯이 데이터 건수가 적은 경우에는 최적화 의미가 없다.
- 데이터 건수가 ‘적다’는 걸 사람의 직감으로 추측하는것도 좋지 않다.

### 써드 파티를 잘 활용하자

---

- **제 3자가 이용하기 쉽도록 이미 구현된 소스를 활용하는 것도 좋다. 이런 종류의 소스를 잘 활용하면 공수를 줄일 수 있다.**
- 대신 어느 정도는 **구현내용을 파악하는** 것이좋다.
    - 라이브러리는 API가 우리가 원하는 사양으로 되어 있지 않다거나 불필요한 구현이 너무 많아 다소 오버스펙인 경우가 있다.
- 필요로 하는 부분만을 구현하여 공수를 줄이고 비용 대비 효과도 높이면 좋다.

### 데이터 압축과 속도 : 전체적인 처리량을 높이기 위한 사고방식

---

- 큰 파일을 압축시킬 때 머신 파워를 사용한다.
    - 이런 이미지로부터 ‘압축해제’ 처리는 무겁다=느리다라는 이미지가 존재한다.
- 하지만 처리량 관점에서는 다루는 데이터를 압축해두는 편이 빠른 경우가 있다.
    - 컴퓨터에는 CPU와 I/O라는 두 종류의 부하가 있다. 어느 특정 처리를 하면서 I/O를 기다리고 있는 동안은 해당 처리에서 CPU를 처리할 수 없다.
- **파일을 압축해두면 CPU에 조금이 부담이 되지만 I/O를 대기를 줄일 수 있다. CPU가 여유 있고 I/O가 바쁜 경우는 많으므로 압축으로 인해 I/O를 줄이고 CPU에 떠맡김으로써 전체적인 처리량을 올린다.**
    - ex ) HTTP deflate 압축통신이 좋은 예
- 압축은 중요한 기술 중 하나.

## 하테나 다이어리의 키워드 링크 : [사이트 링크](https://d.hatena.ne.jp/)

### 키워드 링크란?

---

![스크린샷 2023-07-25 오후 3.42.17.png](%5BChapter%207%5D%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%AA%2080535394562b4b70ba6d26451e8f909c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.42.17.png)

- 위의 사진과 같이 블로그에 글을 작성하면 일부 키워드에 링크가 자동으로 걸린다.
- 이 링크가 걸리는 곳은 이 키워드를 설명하는 페이지로 되어 있다.
- 링크 대상이 되는 키워드는 하테나 키워드에 사용자가 등록한 키워드이다.
- 2009년 8월 기준으로 27만 단어 이상이 등록되어 있으며 대략 하루에 100개 정도 새로운 키워드가 등록되고 있다.
- **입력된 전문에 대해 27만 단어를 포함하는 키워드 사전과 매칭해서 필요한 부분을 링크로 치원하는 것이 키워드 링크의 기능**

```html
하테나 다이어리는 블로그이다

=> <a href= "---"> 하테나 다이어리 </a> 는 <a href="----"> 블로그 </a> 다.
```

### 최초 구현방법

---

사전 내에 포함된 모든 단어를 OR 조건으로 잇는 정규표현을 만들어 사용하는 방법

```perl
$text = ~ s/(foo|bar|baz) / &replace_keyword($1) / ge;

reb replace_keyword {
	my $w = shift;
	return sprint '<a href = "/keyword/%s"> %s </a>' , uri_escape($w),$w;

}
```

- 코드가 실행되면 문자열에서 "foo", "bar", 또는 "baz"를 찾아 각각 &replace_keyword("foo"), &replace_keyword("bar"), 또는 &replace_keyword("baz")의 반환 값으로 대체

### 문제발생 : 키워드 사전의 대규모화

---

- 정규표현을 컴파일하는 처리
    - 미리 정규표현을 만들어 메모리나 디스크 상에 저장, 즉 캐싱해둠으로써 회피
- 정규표현에서 패턴매칭하는 처리
    - 처음에는 키워드 링크가 완료된 본문 텍스트를 캐싱 , 하지만 새로 키워드가 추가되어 반영해야되면 캐시를 다시 구축해야할 필요가 있거나 혹은 블로그 서비스 특성상 대부분을 차지하는, 그 다지 액세스가 없는 블로그에서는 캐시 효과를 나타내기 어려움 등의 근본적인 해결에는 이르지 못함.
    

### 해결방법 1️⃣ : 정규표현 → Trie 매칭 구현 변경

---

- 패턴매칭에 수반되는 계산량 문제를 해결하기 위해 정규표현을 기반으로 한 방법에서 → **Trie(트라이)를 사용한 매칭 구현으로 변경**
- `트라이` 알고리즘 : 문자열에 특화된 자료 구조인 트라이(Trie)는 문자열 집합을 표현하는 트리 자료구조이며, 원소를 찾는 작업을 O(n)에 해결 할 수 있다.
    
    ![Untitled](%5BChapter%207%5D%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%AA%2080535394562b4b70ba6d26451e8f909c/Untitled.png)
    
- ex ) ab는 “abcde”와 “ab”를 공통 접두사로 갖고 있고 “bab”와 “bc”는 “b”라는 공통 접두사로 하고 있다.
- 공통 접두사를 정리합으로써 불필요한 것을 배제하고 있는 것이 Trie의 특징이다.
- 이전의 정규식 표현 방법과 비교해보자
    - foo, bar, baz라는 키워드를 비교하기 위해서는 모두 매칭되는지를 시도해야되고 키워드수에 비례해서 시간이 길어짐
    - 반대로 hogefoo를 검사한다고 하면 h o g e는 trie에 포함되어있지 않고 f o o 부터 foo가 매칭되니 hogefoo라는 단어의 길이만큼 계산하면 되기 때문에 효과적

### 해결방법 2: Trie 매칭 구현 → AC(Acho - Corasick)법

---

- `AC법` : Trie에서 패턴매칭으로 매칭이 진행되다가 도중에 실패할 경우, **되돌아오는 길의 엣지를 다시  Trie에 추가한 데이터 구조를 사용하는 방법**
    
    ![Untitled](%5BChapter%207%5D%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%AA%2080535394562b4b70ba6d26451e8f909c/Untitled%201.png)
    
- 사전 내에서 패턴 매칭을 수행하는 오토마톤을 구축하고 입력 텍스트에 대해 선형 계산시간을 실현한다.
- 계산량이 사전 크기에 의존하지 않는 빠른 방법
- ex ) babcdes를 입력한 경우 bab가 발견된다음 ab가 발견될꺼다 그러므로 bab까지 탐색했다면 다시 선두 노드인 0로 돌아가는 것이 아닌 노드2로 바로 갈 수 있다는 것을 알 수 있다면 ab를 바로 찾을 수 있을 것이다.
- 여기서 6 다음은 바로 2 라고 길을 내는 전처리를 trie에 대해 수행하는 것이 ac법의 핵심
- 문제는 이 길을 내는 방법인데 ,  trie의 루트부터 너비 우선탐색으로 적당한 노드를 찾아 구성한다.
- AC 방법으로 키둬드 링크의 계산량 문제는 해결

### 해결방법 3 : AC(Acho - Corasick)법  → Regexp::List로 치환

---

- Regexp::List라는 CPAN라이브러리로 치환
- Perl의 정규표현 라이브러리로 , Trie의 기반의 정규 표현을 생성한다.
- **거대한 정규표현을 하테나 다이어리 시초와 같이 OR로 연결 하는 것이 아닌 Trie에 의해 최적화된 정규표현으로 변환하는 라이브러리**

```perl
qw /foobar fooxar foozap fooza/ => foo(?:[bx]ar|zap) 
```

- 공통 접두사나 접미사가 정리되어 있으므로 이 정규표현으로 패턴 매칭을 수행한 경우 , or 로 모든 단어를 연결한거보다 시행 횟수를 큰 폭으로 줄일 수 있음 (Trie 부분과 같음)
- 이점 : 계산량을 줄인것 뿐만아니라 정규표현으로 사용할 수 있다는 점 즉 유연성이 풍부해짐.
    - ac법은 구현으로 변경한 다음에는 그런 이점이 없어져 유연성에는 모자랐다. 이를 사용하므로써 양측의 좋은 점을 모두 취할 수 있게 된 것

### “하테나 다이어리의 키워드 링크” 사례 알아보며…

---

- 정규표현 ( 공수 적고 유연성이 풍부) → 데이터 많아짐 혼재화 발생 → 본질적인 해결책이 필요 → 캐시 등 표면적인 방법으로 문제를 회피하였지만 최종적으로는 **알고리즘이 갖는 근복적인 문제점을 해결해야 했음**
- 이러한 통찰에는 알고리즘의 평가에서 설명했듯이 **계산량 관점에서 문제를 포착**할 필요가 있다.
- **처음부터 최적의 구현을 사용하는 것이 반드시 옳다고는 할 수 없다**.
- **데이터가 대규모가 될 시기를 대비하여 본질적인 문제의 해결방법을 알고 있어야 문제가 발생하여도 해결하기 수월할 것이다**.

## 하테나 북마크 기사 분류

### 기사 분류란?

---

- 새로 도착한 기사를 해당 기사의 내용을 기반으로 자동으로 분류해서 사용자에게 카테고리를 분류해서 보여주는 기능
- 해당 기사를 HTTP로 얻어서 본문 텍스트의 내용으로부터 분류해서 카테고리를 판정

![스크린샷 2023-07-25 오후 6.00.48.png](%5BChapter%207%5D%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%AA%2080535394562b4b70ba6d26451e8f909c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.00.48.png)

### 베이지안 필터

---

- 카테고리 판정에는 베이지안 필터 원리를 이용
- `베이지안 필터` : 텍스트 문서 등을 입력으로 받아들이고 거기에 `나이브 베이즈` 알고리즘을 적용해서 확률적으로 해당 문서가 어느 카테고리에 속하는지를 판정하는 프로그램
    - 미지의 문서의 카테고리 판정을 수행함에 있어서 과거에 분류가 끝난 데이터의 통계정보로 부터 판정을 수행
    - 정답이 정해진 ‘정해 데이터’를 주고 프로그램을 학습 시켜두면 최종적으로 수동으로 개입하지 않아도 정해를 알 수 있다.

### 나이브 베이즈(Naive Bayes)알고리즘

---

- `나이브 베이즈` : 알고리즘은 확률적인 원리를 기반으로 하며, 특히 분류 작업에 많이 사용되는 머신 러닝 알고리즘
    
    ![스크린샷 2023-07-25 오후 6.12.16.png](%5BChapter%207%5D%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%AA%2080535394562b4b70ba6d26451e8f909c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.12.16.png)
    
- 문서  D가 주어졌을때 카테고리 C인 조건부 확률을 구하는 문제  ⇒   P(C|D)
    - ‘베이즈 정리’ 로 인해 해당 식을 `P(C|D) = P(D|C) * P(C) / P(D)` 변형가능
        - P(D) : **문서가 발생확률**인데 이는 모든 카테고리에 대해 동일한 값으로 결과를 비교할 경우에 무시가능
        - P(D|C): 문서 D라는 것은 임의의 단어 W가 연속해서 출현하는 것으로 간주하고 → 단어를 W라고 하였을 때 P(W1|C) P(W2|C) P(W3|C) ….. P(Wn|C)와 같은 식으로 변형 가능. 문서 D를 단어로 분할해두고 그 단어마다 어느 카테고리로 분류됐는지 그 횟수를 보존하면 P(D|C)의 근사값을 구할 수 있다.
        - P(C) : **특정 카테고리가 출현할 확률**, 학습 데이터 중 여러 데이터가 어떤 카테고리로 분류되었는지 그 횟수를 저장해두면 계산가능
    

### 알고리즘이 실용화되기까지 : 하테나 북마크의 실제 사례

---

- 베이지안 필터로 만든 카테고리 분류 엔진을 프로덕션 환경에 올리기 위한 작업 사례
    1. 분류 엔진 C++로 개발하여 이를 서버화한다.
    2. 이 서버와 통신해서 결과를 얻는 Perl 클라이언트를 작성하고 웹 애플리케이션에서 호출한다.
    3. 학습 데이터를 정기적으로 백업할 수 있도록 C++엔진에 데이터 덤프/로드 기능을 추가한다.
    4. 학습 데이터 1,000을 수작업으로 준비한다.  이 부분은 사람이 하게 된다.
    5. 바람직한 정밀도가 나오는지를 추적하기 위한 통계 구조를 작성한다. 그래프화하면서 정밀도를 튜닝한다.
    6. 다중화를 고려해서 스탠바이 시스템을 구축한다. 자동 페일오버(fail-over)는 역시나 공수가 많이 소요되므로 백업에서 로드할 수 있는 정도로 타협한다.
    7. 웹 애플리케이션에 사용자 인터페이스를 마련한다.
- **위의 과정을 통해 실제로 알고리즘을 실용화하기까지는 실무 면에서 고려해야할 점이 어느정도는 존재한다는 것을 알 수 있다.**

### 수비 자세 , 공격 자세 : 기사 분류 구현으로부터의 고찰

---

- `수비 자세` : 대규모 데이터를 빠르게 정렬하거나 검색, 압축하는 일은 발생하는 문제를 얼마나 잘 맞아들이는가?
- `공격 자세` : 기계학습이나 패턴인식 등은 적극적으로 대규모 데이터를 응용하고 그 결과에 따라 애플리케이션에 부가가치를 추가한다는 자세로 사용되는 알고리즘
- **수비를 하던 공격을 하던 대규모 데이터에 대해 알고리즘 측면에서의 접근방법을 배우려고 할 때에는 기존 방법은 어느 정도 자신의 지식으로 익히는 것이 중요하다.**
    - 키워드 링크에 Trie를 응용할 수 있다는 발상은 Trie가 어떤 데이터 구조인지 그 특성을 모르면 생각해 낼 수 없었을 것이다.
    - 베이지안 필터와 같은 원리를 이해해두지 않았다면 문서를 자동으로 분류한다는 발상을 할 수 없었을 것이다.
- 대량 데이터에 맞서 알고리즘을 선택하고 이를 응용하는 것이 어떤 것인지 감각을 익혀두는것이 중요하다.

## 추가 조사

- `증분 검색` : 컴퓨터에서 사용자가 글자를 입력하는 도중에 계속적으로 해당하는 내용을 찾아주는 기능
- `접미사 배열 (Suffix Array)` : 문자열 S의 모든 접미사를 사전순으로 정렬해 놓은 배열
    
    ![Untitled](%5BChapter%207%5D%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%AA%2080535394562b4b70ba6d26451e8f909c/Untitled%202.png)
    
- `오토마톤` : 오토마톤(Automaton)이란 입력에 따라 정해진 규칙에 따라 동작하는 자동화된 시스템 또는 기계를 의미합니다

## 참고

[](https://zdnet.co.kr/view/?no=20220725093548)

[접미사 배열(Suffix Array)](https://zoosso.tistory.com/292)

[[알고리즘] 아호 코라식(Aho-Corasick) 알고리즘](https://pangtrue.tistory.com/305)